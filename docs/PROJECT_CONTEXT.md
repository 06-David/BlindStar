# 项目背景与技术栈

## 🎯 项目背景

BlindStar 智能视觉辅助系统旨在通过先进的计算机视觉和人工智能技术，为视障人士提供一个更安全、更便捷的移动和感知环境。在日常生活中，视障人士面临着诸多挑战，例如识别障碍物、理解周围环境、获取地点信息以及进行独立导航。传统的辅助工具往往功能单一，难以提供全面的环境感知能力。

本项目正是在这样的背景下应运而生，致力于开发一个集成多模态感知能力的智能系统，帮助视障用户“看清”世界，提升其自主生活的能力和信心。

## ✨ 项目目标

- **实时环境感知**：通过摄像头输入，实时检测和识别周围的物体、行人、交通状况。
- **精确距离与运动评估**：提供物体与用户之间的距离信息，并分析物体的运动方向和速度。
- **智能语音交互**：支持自然语言语音命令，实现免手动操作，提供语音反馈。
- **高精度导航辅助**：结合高德地图 API，提供路线规划、实时导航指引和兴趣点查询。
- **模块化与可扩展性**：采用清晰的模块化架构，方便功能的迭代开发和新技术的集成。
- **离线能力**：核心功能支持离线运行，保障用户在无网络环境下的使用体验和数据隐私。

## 🛠️ 核心技术栈

BlindStar 系统融合了以下前沿技术：

- **深度学习框架**:
  - **YOLOv8 / YOLO11 (Ultralytics)**: 用于高性能的实时物体检测，能够识别并分类图像中的多种目标。
  - **ZoeDepth**: 业界领先的单目深度估计模型，提供精确的物体距离测量，弥补了传统 MiDaS 模型在某些场景的不足，并支持离线推理。

- **计算机视觉库**:
  - **OpenCV**: 强大的图像和视频处理库，用于帧的捕获、处理、标注以及视频的写入。
  - **PIL (Pillow)**: Python 图像处理库，在某些模型预处理中用于图像操作。

- **语音技术**:
  - **Vosk**: 轻量级、离线的语音识别引擎，支持高准确度的中文语音识别，确保用户隐私和在无网络环境下的语音命令处理。
  - **pyttsx3**: 一个跨平台的文本转语音库，提供基本的语音合成功能。
  - **Windows SAPI**: 在 Windows 系统上优先使用的语音合成接口，提供更稳定和自然的语音输出。

- **地图与导航**:
  - **高德地图 API**: 提供丰富的地理信息服务，包括POI（兴趣点）搜索、路径规划和实时导航。

- **数据处理与科学计算**:
  - **NumPy**: 进行高效的数值计算，处理图像数据和深度图。
  - **Pandas**: 用于结构化数据的处理和分析，例如处理帧分析的 CSV 日志。

- **系统设计与并发**:
  - **Python `threading` 和 `queue` 模块**: 实现多线程并发处理，确保实时性分析和语音交互的流畅性，避免主程序阻塞。

通过这些技术的集成，BlindStar 旨在为视障用户构建一个功能强大、响应迅速且高度智能化的视觉辅助解决方案。

## 🧱 系统总体架构

```
应用入口层
├── main.py                  # CLI/应用入口（计划/可选）
├── realtime_launcher.py     # 统一启动器（推荐入口）
└── batch_process.py         # 批量处理工具（可选）

核心功能层 core/
├── detector.py              # 物体检测（YOLO11/YOLOv8）
├── distance.py              # 深度估计/距离计算（ZoeDepth/MiDaS）
├── depth_hazard_detector.py # 深度危险检测（台阶/悬崖/障碍）
├── risk_assessor.py         # 风险评估（多维度融合）
├── decision_engine.py       # 决策引擎（策略选择）
├── action_planner.py        # 行动规划（避障动作）
├── object_tracker.py        # 目标追踪/轨迹预测
├── camera.py                # 相机/视频源管理
├── tts_engine.py            # 文本转语音（SAPI/pyttsx3）
├── stt_engine.py            # 语音识别（Vosk）
├── amap_navigation.py       # 高德地图集成（POI/路径/导航）
└── utils.py                 # 工具与通用组件

配置与模型
├── config.py                # 全局配置（多模块参数）
└── models/                  # 权重与模型文件
```

相关模块详见：
- 检测模块: `docs/检测模块.md`
- 深度估计: `docs/深度估计模块.md`
- 避障模块: `docs/避障模块.md`
- 相机模块: `docs/相机模块.md`
- 系统集成: `docs/系统集成模块.md`

## 🧩 模块职责与协同

- **检测器 `core/detector.py`**：从帧中产出检测框、类别与置信度。
- **深度估计 `core/distance.py`**：生成深度图，计算指定bbox的距离，融合尺寸/深度等多源估计。
- **危险检测 `core/depth_hazard_detector.py`**：基于深度的悬崖、台阶、近距离障碍识别。
- **风险评估 `core/risk_assessor.py`**：融合距离、速度、轨迹、物体类型，输出L0-L3风险。
- **决策引擎 `core/decision_engine.py`**：依据风险与场景上下文生成策略（继续/减速/变向/停止…）。
- **行动规划 `core/action_planner.py`**：把策略转为可执行动作序列（方向、速度、等待）。
- **语音交互 `core/tts_engine.py`/`core/stt_engine.py`**：播报环境与指令，接收语音命令。
- **导航 `core/amap_navigation.py`**：POI/路径/逐步导航，向语音与决策提供上下文。
- **相机 `core/camera.py`**：多源视频输入、分辨率/帧率控制、缓冲管理。

## 🔀 关键数据流与接口

1. 帧采集 → `camera` 输出 BGR 图像矩阵。
2. 物体检测 → `detector` 输出 detections 列表。
3. 深度估计 → `distance` 输出 depth_map，并对bbox求距离。
4. 风险评估 → `risk_assessor` 基于距离/速度/轨迹合成风险分值与等级。
5. 决策与行动 → `decision_engine` → `action_planner` 产出动作与语音提示。
6. 语音播报/导航更新 → `tts_engine` 播报，`amap_navigation` 更新路径上下文。

示例检测结果（统一结构）：
```json
{
  "bbox": [x1, y1, x2, y2],
  "class": "person",
  "class_id": 0,
  "confidence": 0.87
}
```

距离/融合结果：
```json
{
  "bbox": [x1, y1, x2, y2],
  "class": "person",
  "distance_m": 2.35,
  "method": "fused",  
  "components": {"depth_based": 2.3, "size_based": 2.6}
}
```

风险评估输出：
```json
{
  "object_id": 12,
  "risk_score": 0.72,
  "risk_level": "L3_DANGER",
  "factors": {"distance": 0.8, "speed": 0.6, "trajectory": 0.7}
}
```

## 🚀 运行模式与入口

- `full`：检测+深度+语音+导航+相机，完整体验。
- `detection_only`：仅检测与相机，用于快速验证视觉链路。
- `voice_navigation`：仅语音与导航，用于室内外导航辅助。
- `offline`：离线检测+深度+本地语音，适配无网场景。

推荐入口：`realtime_launcher.py`（详见 `docs/系统集成模块.md`）

## ⚙️ 配置与环境

- 配置文件：`config.py`（详见 `docs/系统集成模块.md`/`docs/README.md`）
- 关键环境变量：
  - `AMAP_API_KEY`：高德地图API密钥
  - `BLINDSTAR_MIDAS_DIR`：MiDaS离线模型目录（可选）
- 硬件建议：Windows + NVIDIA GPU（CUDA），或CPU降级模式

## 📑 接口与约定（面向模块/AI代理）

- 图像：OpenCV BGR `np.ndarray(H, W, 3)`；深度图：`np.ndarray(H, W)` 浮点米制。
- 坐标：`bbox=[x1,y1,x2,y2]` 像素坐标，左上为原点。
- 时间：所有模块尽量附带 `timestamp`（秒）。
- 线程：耗时任务（检测/深度/语音）建议各自线程+队列。
- 错误：模块抛出带上下文信息的异常消息，供上层TTS友好转述。

## 📈 日志与监控

- 日志级别：DEBUG/INFO/WARNING/ERROR；关键路径打印耗时与FPS。
- 监控指标：
  - 视觉：检测耗时、深度耗时、整体FPS
  - 交互：STT/TTS延迟
  - 导航：路径刷新耗时、定位可用性

## 🧪 测试与验证

- 单元测试（计划/进行中）：关键函数的输入输出稳定性
- 回归样例：典型场景视频与截图基线结果
- 实地走测：室内/室外/夜间/逆光/拥挤等场景

## 🗺️ 路线图（Roadmap）

- v1.3（当前）：
  - ZoeDepth集成、MiDaS备用、YOLO11检测、自定义模型
  - L0-L3风险评估、决策与行动规划、语音/导航集成
  - 文档中心与模块化文档
- v1.4（计划）：
  - 多目标轨迹预测优化与碰撞时间估计
  - 更稳健的台阶/悬崖检测与地面分割
  - 模块化配置热更新与可视化调参面板
  - 模型部署优化（TensorRT/ONNX Runtime 可选）

## 🤖 给AI代理的速览提示

- 入口：优先从 `realtime_launcher.py` 与 `core/` 各模块入手。
- 任务：根据用户需求选择相应模块（检测/深度/导航/语音/避障）。
- 语境：读取 `config.py` 与 `docs/README.md` 获取当前启用的参数与模式。
- 输出：遵循“检测→距离→风险→决策→动作/语音”的流水线结构。